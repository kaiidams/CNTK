{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "# CNTK 206 Part C: Wasserstein and Loss Sensitive GAN with CIFAR Data\n",
     "\n",
     "**Prerequisites**: We assume that you have successfully downloaded the CIFAR data by completing tutorial CNTK 201A. Or you can run the CNTK 201A image data downloader notebook to download and prepare CIFAR dataset.\n",
     "\n",
     "**Contributed by**: [Anqi Li](https://www.linkedin.com/in/anqi-li-4a02b1103/) October 17, 2017 \n",
     "\n",
     "## Introduction\n",
     "Generative models have gained a lot of attention in deep learning community which has traditionally leveraged discriminative  models for semi-supervised and unsupervised learning. [Generative Adversarial Network (GAN)](https://arxiv.org/pdf/1406.2661v1.pdf) (Goodfellow *et al.*, 2014) is one of the most popular generative model because of its promising results in [various tasks](https://github.com/HKCaesar/really-awesome-gan) in computer vision and natural language processing. However, the original version of GANs are notoriously difficult to train. Without carefully-chosen hyper-parameters and network architecture that balances Generator and Discriminator training, GANs could easily suffer from vanishing gradient or mode collapse (where the model is only able to produce a single or a few samples). In this tutorial, we introduce several improved GAN models, namely [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf) (W-GAN) (Arjovsky *et al.*, 2017) and [Loss Sensitive GAN](https://arxiv.org/pdf/1701.06264.pdf) (LS-GAN) (Qi, 2017), that are proposed to address the problems of vanishing gradient and mode collapse.\n",
     "\n",
     "## Overview\n",
     "In this section, we hightlight the key differences between Wasserstein GAN and original GANs (shown in CNTK 206 A and B tutorials) both theoretically as well as from an implementation perspective.\n",
     "\n",
     "### Why is GAN hard to train?\n",
     "In the training of the original GANs, balancing the convergence of the discriminator and the generator is extremely important because if one is far ahead of the other, the other cannot get enough gradient to improve. However, balancing the convergence of two neural networks is hard.\n",
     "\n",
     "The mathematical details are summarized here and in [this paper here](https://arxiv.org/pdf/1701.04862.pdf). You may skip the math and look at the implementation details for W-GAN and LS-GAN.\n",
     "\n",
     "A [typical GAN](https://arxiv.org/pdf/1406.2661v1.pdf) includes two neural network, a *Generator* $G$ and a *Discriminator* $D$. The training of GAN is modeled as a two-player zero-sum game. The Discriminator D is trained to predict the probability that a sample is a real sample rather than generated from the generator G, while the generator G is trained to better fool the discriminator by producing real-looking samples. The objective (also referred to as the value function) for GAN training is,\n",
     "\n",
     "$$\\min_G\\max_D V(D,G)=\\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(x)}[\\log(1-D(G(z)))]$$\n",
     "\n",
     "In the original GAN paper, the author proves that the optimal strategy for discriminator is predicting\n",
     "\n",
     "$$D^*(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_{model}(x)}$$\n",
     "\n",
     "By plugging it into the GAN objective function, one may find that the discriminator is actually an estimation of *Jensen-Shannon* divergence (JS divergence or JSD) of two distributions (data and model).\n",
     "\n",
     "$$L(D^*,g_\\theta)=2 JSD(\\mathbb{P}_{data}\\|\\mathbb{P}_{model}) - 2\\log2$$\n",
     "\n",
     "This implies that the optimal strategy is when $$\\mathbb{P}_{data}= \\mathbb{P}_{model}$$.  However, JS distance may become locally saturated and gets vanishing gradient to train the GAN generator if the discriminator is over-trained. Also JSD is a positive value. \n",
     "\n",
     "\n",
     "### Wasserstein GAN\n",
     "To address this problem, [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf) was proposed to use a different distance measurement for probability distributions, namely *Earth-Mover* (EM) distance or *Wasserstein* distance instead of JS divergence. The authors claimed that by using EM distance, one no longer needs to carefully maintain the balance between the generator and the discriminator, and, notably, the output of the discriminator is referred as the critic instead. EM distance serves as a good indicator of image quality of generated samples. The EM distance of two distribution is defined as\n",
     "\n",
     "$$W(p_{data}, p_{model})=\\inf_{\\gamma\\in\\prod(p_{data},p_{model})}\\mathbb{E}_{(x,y)\\sim\\gamma}\\left[\\|x-y\\|\\right]$$\n",
     "\n",
     "EM distance is a more sensible distance measurement than JS divergence since EM distance is continuous and differentiable anywhere while JS divergence is not. The authors uses the Kantorovich-Rubinstein duality to derive the objective for Wasserstein GAN,\n",
     "\n",
     "$$\\min_G\\max_{\\|D\\|_L\\leq K} \\mathbb{E}_{x\\sim p_{data}(x)}[D(x)] - \\mathbb{E}_{z\\sim p_z(x)}[D(G(z))]$$\n",
     "\n",
     "**Note**: The Kantorovich-Rubinstein duality requires the function to be K-Lipschitz. The authors suggest *clipping the weights of discriminator* to satisfy [Lipschitz continuity](https://en.wikipedia.org/wiki/Lipschitz_continuity).\n",
     "\n",
     "#### Implementation details\n",
     "\n",
     "The modification needed on implementation side is minor. One can change an original GAN into a Wasserstein GAN with a few lines of code:\n",
     "\n",
     "1. Use W-GAN loss function\n",
     "2. Remove the sigmoid activation for the last layer of discriminator\n",
     "3. Clip the weights of the discriminator after updates (e.g., to [-0.01, 0.01])\n",
     "4. Train discriminator more iterations than generator (e.g., train the discriminator for 5 iterations and train the generator for one iteration only at each round)\n",
     "5. Use Adam with `momentum=0`\n",
     "6. Use small learning rate (e.g., 0.00005)\n",
     "\n",
     "### Loss Sensitive GAN\n",
     "\n",
     "[Loss Sensitive GAN](https://arxiv.org/pdf/1701.06264.pdf) was proposed to address the problem of vanishing gradient. LS-GAN is trained on a loss function that allows the generator to focus on improving poor generated samples that are far from the real sample manifold. The author shows that the loss learned by LS-GAN has non-vanishing gradient almost everywhere, even when the discriminator is over-trained.\n",
     "\n",
     "$$\\min_D L_D = \\mathbb{E}_{x\\sim p_{data}(x)}[D(x)] + \\lambda\\mathbb{E}_{x\\sim p_{data}(x), z\\sim p_z(x)}\\left[\\left(\\|x-G(z)\\|_1 + D(x) - D(G(z))\\right)_+\\right]$$\n",
     "$$\\min_G L_G = \\mathbb{E}_{z\\sim p_z(x)}[D(G(z))]$$\n",
     "\n",
     "#### Implementation details\n",
     "\n",
     "The modification needed on implementation side is also minor. On can change an original GAN into a Wasserstein GAN with a few lines of code:\n",
     "\n",
     "1. Use the LS-GAN loss function\n",
     "2. Remove the sigmoid activation for the last layer of discriminator\n",
     "3. Update both the generator and the discriminator with weight decay\n",
     "4. Train discriminator and generator each with one iteration at each round\n"
    ]
   },
   "source": [
    "# CNTK 206 パート C: CIFAR データを用いたWassersteinと損失感受性 GAN\n",
    "\n",
    "**前提条件**: チュートリアル CNTK 201A を完了して、CIFAR データを正常にダウンロードしたことを前提としています。または、CNTK 201A 画像データダウンローダのノートブックを実行して、CIFAR データセットをダウンロードして準備することもできます。\n",
    "\n",
    "**貢献者**: [Anqi Li](https://www.linkedin.com/in/anqi-li-4a02b1103/)2017年10月17日\n",
    "\n",
    "## 序論\n",
    "\n",
    "深層学習コミュニティでは、これまで伝統的に(半教師) と教師なし学習用の識別モデルを活用してきましたが、生成モデルが多くの注目を集めるようになってきました。[敵対的生成ネットワーク (GAN)](https://arxiv.org/pdf/1406.2661v1.pdf)(2014 Goodfellow*ら*) は、コンピュータビジョンと自然言語処理における[様々なタスク](https://github.com/HKCaesar/really-awesome-gan)でのすばらしい結果のため最も人気のある生成モデルの一つです。しかし、元々のGANはひどく学習が困難でした。生成器と弁別器の学習のバランスが取れた慎重に選択されたハイパーパラメータとネットワークアーキテクチャなしには、GANは簡単に勾配消失やモード崩壊 (モデルが単一またはいくつかのサンプルしか生成することができない) に陥ってしまいます。本チュートリアルでは、勾配消失とモード崩壊の問題に対処するために提案された、いくつかの改良された GAN モデル、すなわち[Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf) (W-GAN) (2017 Arjovsky *ら*) と[損失感受性 GAN](https://arxiv.org/pdf/1701.06264.pdf) (LS-GAN) (2017 Qi) を紹介します。\n",
    "\n",
    "## 概要\n",
    "\n",
    "このセクションでは、Wasserstein GAN と元々のGAN (CNTK 206 A と B のチュートリアルに掲載されている) の主な違いを、理論的かつ実装の観点にから光を当てます。\n",
    "\n",
    "### なぜ、GAN は訓練しにくいのか？\n",
    "\n",
    "元々のGANの訓練では、弁別器と生成器の収束のバランスが非常に重要でした。片方がずっと先まで先行してしまった場合、他方は改善するために十分な勾配を得ることができません。しかし、2つのニューラルネットワークの収束のバランスをとるのは困難です。\n",
    "\n",
    "[この論文](https://arxiv.org/pdf/1701.04862.pdf)のように、ここでは数学的な詳細について説明します。数学をスキップして、W-GAN と LS-GAN の実装の詳細を見てもよいでしょう。\n",
    "\n",
    "[典型的な GAN](https://arxiv.org/pdf/1406.2661v1.pdf)には、2つのニューラルネットワーク、*生成器* $ G $ 、および*弁別器* $ D $が含まれます。GAN の訓練は、二人のプレーヤーのゼロサムゲームとしてモデル化されています。この弁別器 D は、サンプルが生成器 G から生成されたのではなく実際のサンプルである確率を予測するために訓練されており、生成器 G は本物らしいサンプルを生成することにより、できる限り弁別器をだますように訓練されています。GAN 学習の目的 (評価関数とも呼ばれます) は、\n",
    "\n",
    "$$\\min_G\\max_D V(D,G)=\\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(x)}[\\log(1-D(G(z)))]$$\n",
    "\n",
    "です。元々の GAN の論文では、著者は、弁別器にとって最適な戦略は\n",
    "\n",
    "$$D^*(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_{model}(x)}$$\n",
    "\n",
    "であることを証明しています。それを GAN の目的関数に入れ込むことで、実際には弁別器は、2つの分布 (データとモデル) の*Jensen-Shannon*ダイバージェンス (JS ダイバージェンスまたは JSD) の推定であることがわかります。\n",
    "\n",
    "$$L(D^*,g_\\theta)=2 JSD(\\mathbb{P}_{data}\\|\\mathbb{P}_{model}) - 2\\log2$$\n",
    "\n",
    "これは、$$\\mathbb{P}_{data}= \\mathbb{P}_{model}$$のときに最適な戦略であることを意味します。 しかし、弁別器が過剰に訓練されている場合、JS 距離は局所的に飽和状態になることがあり、勾配消失になってしまう可能性があります。また、JSD は正の値です。\n",
    "\n",
    "### Wasserstein GAN\n",
    "\n",
    "この問題に対処するために、[Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)では、確率分布の距離を測るために別の方法を提案しました。すなわち*Earth-Mover* (EM) 距離、または、 JS ダイバージェンスの代わりの*Wasserstein*距離です。著者らは、EM 距離を使用すれば、もはや慎重に生成器と弁別器の間のバランスを維持する必要がないと主張し、特に、弁別器の出力は、酷評であるとまで言いました。EM 距離は、生成されたサンプルの画質の良い指標として機能します。2つの分布の EM 距離は、\n",
    "\n",
    "$$W(p_{data}, p_{model})=\\inf_{\\gamma\\in\\prod(p_{data},p_{model})}\\mathbb{E}_{(x,y)\\sim\\gamma}\\left[\\|x-y\\|\\right]$$\n",
    "\n",
    "と定義されます。JSダイバージェンスと違い、EM 距離はどこでも連続で微分可能であるので、JSダイバージェンスよりも意味のある距離の測定方法です。著者らは、Kantorovich-Rubinsteinの二重性を使って、Wasserstein GAN の目的関数を導出しました。 \n",
    "\n",
    "$$\\min_G\\max_{\\|D\\|_L\\leq K} \\mathbb{E}_{x\\sim p_{data}(x)}[D(x)] - \\mathbb{E}_{z\\sim p_z(x)}[D(G(z))]$$\n",
    "\n",
    "**注**: Kantorovich-Rubinsteinの二重性は、関数が Kリプシッツである必要があります。著者らは、[リプシッツ連続性](https://en.wikipedia.org/wiki/Lipschitz_continuity)を満たすために*弁別器の重みをクリッピング*することを推奨しています。\n",
    "\n",
    "#### 実装の詳細\n",
    "\n",
    "実装側で必要な変更はごくわずかです。数行のコードで元々の GAN をWasserstein GAN に変更することができます:\n",
    "\n",
    "1. W-GAN 損失関数を使う\n",
    "2. 弁別器の最後の層のシグモイド活性化関数を削除します。\n",
    "3. 更新後の弁別器の重みをクリッピングする (例: [-0.01, 0.01])\n",
    "4. 生成器よりも多くの弁別器を訓練する (例えば、各ラウンドで５回弁別器を訓練し1回のみ生成器を訓練する)\n",
    "5. `momentum=0`のAdam を使う\n",
    "6. 小さな学習率を使用する (例: 0.00005)\n",
    "\n",
    "### 損失感受性 GAN\n",
    "\n",
    "[損失感受性 GAN](https://arxiv.org/pdf/1701.06264.pdf)は勾配消失の問題に対処するために提案されました。LS-GAN は、生成器は、実際のさまざまなサンプルから、かけ離れた稚拙なサンプルのみを改善することに集中することができる損失関数で訓練されています。著者は、弁別器が過剰に訓練されている場合でも、LS-GAN によって学んだ損失関数は、ほとんどどこにも勾配消失がないことを示しています。\n",
    "\n",
    "$$\\min_D L_D = \\mathbb{E}_{x\\sim p_{data}(x)}[D(x)] + \\lambda\\mathbb{E}_{x\\sim p_{data}(x), z\\sim p_z(x)}\\left[\\left(\\|x-G(z)\\|_1 + D(x) - D(G(z))\\right)_+\\right]$$\n",
    "$$\\min_G L_G = \\mathbb{E}_{z\\sim p_z(x)}[D(G(z))]$$\n",
    "\n",
    "#### 実装の詳細\n",
    "\n",
    "これも実装側で必要な変更は軽微です。数行のコードで元々の GAN を 損失感受性 GAN に変更することができます:\n",
    "\n",
    "1. LS-GAN 損失関数を使用する\n",
    "2. 弁別器の最後の層のシグモイド活性化関数を削除します\n",
    "3. 生成器と弁別器の両方を重み減衰で更新します\n",
    "4. 各ラウンドで1回の反復で弁別器と生成器を訓練します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "original_source": [
     "There are two run modes:\n",
     "*Fast mode*: `isFast` is set to `True`. This is the default mode for the notebooks, which means we train for fewer iterations or train / test on limited data. This ensures functional correctness of the notebook though the models produced are far from what a completed training would produce.\n",
     "*Slow mode*: We recommend the user to set this flag to `False` once the user has gained familiarity with the notebook content and wants to gain insight from running the notebooks for a longer period with different parameters for training.\n",
     "\n",
     "**Note**: If the `isFlag` is set to `False` the notebook will take hours or even days on a GPU enabled machine. You can try fewer iterations by setting the `num_minibatches` to a smaller number which comes at the expense of quality of the generated images."
    ]
   },
   "source": [
    "実行モードは二つあります:\n",
    "\n",
    "- *高速モード*: `isFast` は`True`に設定されています。これは、ノートブックのデフォルトのモードで、少ない反復回数で学習または、限られたデータで学習/テストすることを意味します。できたモデルは完全な学習によるものからはほど遠いですが、こうすることによってノートブックの機能の正しさを保証できます。\n",
    "\n",
    "- *低速モード*: ユーザーがノートブックの内容に慣れ親しんだ後で、学習のためのさまざまなパラメータを変えたり、ノートブックをもっと長期間実行したりして深い洞察を得たくなったときに、このフラグを`False`に設定することをお勧めします。\n",
    "\n",
    "**注**\n",
    "`isFlag`が `False` に設定されている場合、ノートブックは GPU ありのマシンでも数時間から数日かかります。`num_minibatches`を小さい数値に設定することで、反復回数を減らすことができますが、その場合、生成された画像の品質は犠牲になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isFast = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "## Data Reading\n",
     "The input to the GANs will be a vector of random numbers. At the end of the training, the GAN \"learns\" to generate images drawn from the CIFAR dataset. We will be using the same CIFAR data prepared in tutorial CNTK 201A. For our purposes, you only need to know that the following function returns an object that will be used to read images from the CIFAR dataset. "
    ]
   },
   "source": [
    "## データの読み取り\n",
    "\n",
    "GANへの入力はランダムな数字のベクトルになります。トレーニングの終了時に、GAN は CIFAR データセットから取り出された画像を生成するように「学習」します。ここではチュートリアル CNTK 201A で準備したのと同じ CIFAR データを使用します。とりあえずは、次の関数が CIFAR データセットから画像を読み取るために使用するオブジェクトを返すことを気に留めておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image dimensionalities\n",
    "img_h, img_w = 32, 32\n",
    "img_c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the data path for testing\n",
    "# Check for an environment variable defined in CNTK's test infrastructure\n",
    "envvar = 'CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY'\n",
    "def is_test(): return envvar in os.environ\n",
    "\n",
    "if is_test():\n",
    "    data_path = os.path.join(os.environ[envvar],'Image','CIFAR','v0','tutorial201')\n",
    "    data_path = os.path.normpath(data_path)\n",
    "else:\n",
    "    data_path = os.path.join('data', 'CIFAR-10')\n",
    "    \n",
    "train_file = os.path.join(data_path, 'train_map.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(map_file, train):\n",
    "    print(\"Reading map file:\", map_file)\n",
    "    \n",
    "    if not os.path.exists(map_file):\n",
    "        raise RuntimeError(\"This tutorials depends 201A tutorials, please run 201A first.\")\n",
    "    \n",
    "    import cntk.io.transforms as xforms\n",
    "    transforms = [xforms.crop(crop_type='center', side_ratio=0.8),\n",
    "                  xforms.scale(width=img_w, height=img_h, channels=img_c, interpolations='linear')]\n",
    "    # deserializer\n",
    "    return C.io.MinibatchSource(C.io.ImageDeserializer(map_file, C.io.StreamDefs(\n",
    "        features = C.io.StreamDef(field='image', transforms=transforms), # first column in map file is referred to as 'image'\n",
    "        labels   = C.io.StreamDef(field='label', shape=10)      # and second as 'label'\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noise_sample(num_samples):\n",
    "    return np.random.uniform(\n",
    "        low = -1.0,\n",
    "        high = 1.0,\n",
    "        size = [num_samples, g_input_dim]\n",
    "    ).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "## W-GAN Implementation\n",
     "\n",
     "Note that we assume that you have already completed the DCGAN tutorial. If you need a basic recap of GAN concepts or DCGAN architecture, please visit our [DCGAN tutorial](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_206B_DCGAN.ipynb). \n",
     "### Model Configuration\n",
     "We implemented the W-GAN based on [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) architecture. In this step, we define some of the architectural and training hyper-parameters for our model.\n",
     "* The generator is convolutional transpose layer with $5\\times5$ kernels and strides of $2$\n",
     "* The input of the generator is a 100-dimensional random vector\n",
     "* The output of the generator is a flattened $64\\times64$ image with $3$ channels\n",
     "* The discriminator is a convolutional layer with $5\\times5$ kernels and strides of $2$\n",
     "* The input of the discriminator is also a flattened $64\\times64$ image with $3$ chann"
    ]
   },
   "source": [
    "## W-GAN 実装\n",
    "\n",
    "DCGAN チュートリアルは既に完了していることを想定しています。GAN の概念や DCGAN アーキテクチャの基本的な復習が必要な場合は、[DCGAN チュートリアル](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_206B_DCGAN.ipynb)をご覧ください。\n",
    "\n",
    "### モデル構成\n",
    "\n",
    "[DCGAN](https://arxiv.org/pdf/1511.06434.pdf)アーキテクチャに基づいて、W-GAN を実装しました。このステップでは、モデルのアーキテクチャとトレーニングのいくつかのハイパーパラメータを定義します。\n",
    "\n",
    "- 生成器は$ 5\\times5 $のカーネルと$ 2 $のストライドを持つ畳み込み転置層です。\n",
    "- 生成器の入力は100次元のランダムベクトルです\n",
    "- 生成器の出力は平坦化された$3$チャネルの$ 64\\times64 $の画像です\n",
    "- 弁別器は、 $ 5\\times5 $のカーネルと$ 2 $のストライドを持つ畳み込み層です\n",
    "- 弁別器の入力は同様に平坦化された$ 3 $チャンネルの$ 64\\times64 $の画像です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# architectural hyper-parameters\n",
    "gkernel = dkernel = 5\n",
    "gstride = dstride = 2\n",
    "\n",
    "# Input / Output parameter of Generator and Discriminator\n",
    "g_input_dim = 100\n",
    "g_output_dim = d_input_dim = (img_c, img_h, img_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "We first establish some of the helper functions (batch normalization with relu and batch normalization with leaky relu) that will make our lives easier when defining the generator and the discriminator."
    ]
   },
   "source": [
    "まずいくつかのヘルパー関数を準備します (ReLU付きのバッチ正規化とLeaky ReLU付きのバッチ正規化) これで生成器と弁別器を定義するのが幾分楽になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def bn_with_relu(x, activation=C.relu):\n",
    "    h = C.layers.BatchNormalization(map_rank=1)(x)\n",
    "    return C.relu(h)\n",
    "\n",
    "# We use param-relu function to use a leak=0.2 since CNTK implementation \n",
    "# of Leaky ReLU is fixed to 0.01\n",
    "def bn_with_leaky_relu(x, leak=0.2):\n",
    "    h = C.layers.BatchNormalization(map_rank=1)(x)\n",
    "    r = C.param_relu(C.constant((np.ones(h.shape)*leak).astype(np.float32)), h)\n",
    "    return r\n",
    "\n",
    "def leaky_relu(x, leak=0.2):\n",
    "    return C.param_relu(C.constant((np.ones(x.shape)*leak).astype(np.float32)), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "**Generator**\n",
     "\n",
     "We define the generator according to the DCGAN architecture. The generator takes a 100-dimensional random vector as input and outputs a flattened $3\\times64\\times64$ image. We use convolution transpose layers with relu convolution and batch normalization except for the last layer, where we use tanh to normalize the output to the interval $[-1, 1]$."
    ]
   },
   "source": [
    "**生成器**\n",
    "\n",
    "DCGAN アーキテクチャに従って生成器を定義します。この生成器は、入力として100次元のランダムベクトルを取り、平坦化された$ 3\\times64\\times64 $の画像を出力します。区間$ [-1, 1] $に出力を正規化するために最後の層で使うtanhを除いて、ReLU活性化関数付きのバッチ正規化を用いた畳み込み転置層を使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_generator(z):\n",
    "    with C.layers.default_options(init=C.normal(scale=0.02)):\n",
    "        \n",
    "        gfc_dim = 256\n",
    "        gf_dim = 64\n",
    "        \n",
    "        print('Generator input shape: ', z.shape)\n",
    "        \n",
    "        h0 = C.layers.Dense([gfc_dim, img_h//8, img_w//8], activation=None)(z)\n",
    "        h0 = bn_with_relu(h0)\n",
    "        print('h0 shape', h0.shape)\n",
    "\n",
    "        h1 = C.layers.ConvolutionTranspose2D(gkernel,\n",
    "                                  num_filters=gf_dim*2,\n",
    "                                  strides=gstride,\n",
    "                                  pad=True,\n",
    "                                  output_shape=(img_h//4, img_w//4),\n",
    "                                  activation=None)(h0)\n",
    "        h1 = bn_with_relu(h1)\n",
    "        print('h1 shape', h1.shape)\n",
    "\n",
    "        h2 = C.layers.ConvolutionTranspose2D(gkernel,\n",
    "                                  num_filters=gf_dim,\n",
    "                                  strides=gstride,\n",
    "                                  pad=True,\n",
    "                                  output_shape=(img_h//2, img_w//2),\n",
    "                                  activation=None)(h1)\n",
    "        h2 = bn_with_relu(h2)\n",
    "        print('h2 shape :', h2.shape)\n",
    "        \n",
    "        h3 = C.layers.ConvolutionTranspose2D(gkernel,\n",
    "                                  num_filters=img_c,\n",
    "                                  strides=gstride,\n",
    "                                  pad=True,\n",
    "                                  output_shape=(img_h, img_w),\n",
    "                                  activation=C.tanh)(h2)\n",
    "        print('h3 shape :', h3.shape)\n",
    "\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "**Discriminator**\n",
     "\n",
     "We define the discriminator according to the DCGAN architecture except for the last layer. The discriminator takes a flattened image as input and outputs a single scalar. We do not use any activation at the last layer."
    ]
   },
   "source": [
    "**弁別器**\n",
    "\n",
    "最後の層を除いて DCGAN アーキテクチャに従って弁別器を定義します。弁別器は、平坦化されたイメージを入力として受け取り、単一のスカラーを出力します。最後の層で活性化関数は使われていません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_discriminator(x):\n",
    "    with C.layers.default_options(init=C.normal(scale=0.02)):\n",
    "        \n",
    "        dfc_dim = 256\n",
    "        df_dim = 64\n",
    "        \n",
    "        print('Discriminator convolution input shape', x.shape)\n",
    "\n",
    "        h0 = C.layers.Convolution2D(dkernel, df_dim, strides=dstride, pad=True)(x)\n",
    "        h0 = leaky_relu(h0, leak=0.2)\n",
    "        print('h0 shape :', h0.shape)\n",
    "\n",
    "        h1 = C.layers.Convolution2D(dkernel, df_dim*2, strides=dstride, pad=True)(h0)\n",
    "        h1 = bn_with_leaky_relu(h1, leak=0.2)\n",
    "        print('h1 shape :', h1.shape)\n",
    "\n",
    "        h2 = C.layers.Convolution2D(dkernel, dfc_dim, strides=dstride, pad=True)(h1)\n",
    "        h2 = bn_with_leaky_relu(h2, leak=0.2)\n",
    "        print('h2 shape :', h2.shape)\n",
    "\n",
    "        h3 = C.layers.Dense(1, activation=None)(h2)\n",
    "        print('h3 shape :', h3.shape)\n",
    "\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training config\n",
    "minibatch_size = 64\n",
    "num_minibatches = 500 if isFast else 20000\n",
    "lr = 0.00005 # small learning rates are preferred\n",
    "momentum = 0.0 # momentum is not suggested since it can make W-GANs unstable\n",
    "clip = 0.01 # the weight clipping parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "### Build the graph\n",
     "\n",
     "The discriminator must be used on both the real CIFAR images and fake images generated by the generator function. One way to represent this in the computational graph is to create a clone of the output of the discriminator function, but with substituted inputs. Setting `method=share` in the clone function ensures that both paths through the discriminator model use the same set of parameters\n",
     "\n",
     "We need to update the parameters for the generator and discriminator model separately using the gradients from different loss functions. We can get the parameters for a Function in the graph with the parameters attribute. However, when updating the model parameters, update only the parameters of the respective models while keeping the other parameters unchanged. In other words, when updating the generator we will update only the parameters of the  function while keeping the parameters of the function fixed and vice versa.\n",
     "\n",
     "Because W-GAN needs to clip the weights of the discriminator before every update in order to maintain K-Lipschitz continuity. We build a graph with clipped parameters stored in `clipped_D_params`. The suggested value of clipping threshold is 0.01.\n",
     "\n",
     "**Note**: CNTK parameter learner uses sum of gradient within a minibatch by default instead of mean of gradient. To reproduce  results with the same hyper-parameter in the paper, we need to set `use_mean_gradient = True`, and `unit_gain = False`."
    ]
   },
   "source": [
    "### グラフを作成する\n",
    "\n",
    "この弁別器は、実際の CIFAR 画像と生成関数によって生成した偽の画像の両方で使用される必要があります。計算グラフでこれを表す1つの方法は、弁別関数の入力を置き換えたうえで出力のクローンを作成することです。clone 関数で `method=share` を設定すると、弁別器モデルを通る両方のパスで同じパラメータのセットが使用されるようになります。\n",
    "\n",
    "生成器および弁別器モデルのパラメータは異なる損失関数の勾配で個別に更新する必要があります。parameters属性を使ってグラフ内の関数のパラメータを取得することができます。ただし、モデルのパラメータを更新する場合は、他方のモデルのパラメータを変更せずに、注目するモデルのパラメータのみを更新します。つまり、生成器を更新するときには、その関数のパラメータのみを更新し、弁別関数のパラメータは固定したままにします。逆も同様です。\n",
    "\n",
    "W-GAN は、Kリプシッツ連続性を維持するために、更新の前に毎回、弁別器の重みをクリッピングする必要があるため、`clipped_D_params`に保存されたパラメータのグラフを作成します。クリッピング閾値の推奨値は0.01 です。\n",
    "\n",
    "**注**: CNTK のパラメーター学習器は、既定でミニバッチ内の勾配の平均ではなく、勾配の合計を使用します。同じハイパーパラメータで論文の結果を再現するには、 `use_mean_gradient = True` と `unit_gain = False` を設定する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_WGAN_graph(noise_shape, image_shape, generator, discriminator):\n",
    "    \n",
    "    input_dynamic_axes = [C.Axis.default_batch_axis()]\n",
    "    Z = C.input_variable(noise_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real = C.input_variable(image_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real_scaled = (X_real - 127.5) / 127.5\n",
    "\n",
    "    # Create the model function for the generator and discriminator models\n",
    "    X_fake = generator(Z)\n",
    "    D_real = discriminator(X_real_scaled)\n",
    "    D_fake = D_real.clone(\n",
    "        method = 'share',\n",
    "        substitutions = {X_real_scaled.output: X_fake.output}\n",
    "    )\n",
    "    \n",
    "    clipped_D_params = [C.clip(p, -clip, clip) for p in D_real.parameters]\n",
    "    \n",
    "    G_loss = - D_fake\n",
    "    D_loss = - D_real + D_fake\n",
    "\n",
    "    G_learner = C.adam(\n",
    "            parameters = X_fake.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999),\n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "    )\n",
    "            \n",
    "    D_learner = C.adam(\n",
    "            parameters = D_real.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999),\n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "    )\n",
    "    \n",
    "    # Instantiate the trainers\n",
    "    G_trainer = C.Trainer(X_fake,\n",
    "                        (G_loss, None),\n",
    "                        G_learner)\n",
    "    D_trainer = C.Trainer(D_real,\n",
    "                        (D_loss, None),\n",
    "                        D_learner)\n",
    "\n",
    "    return X_real, X_fake, D_real, clipped_D_params, Z, G_trainer, D_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "### Train the model\n",
     "The code for training the GAN closely follows Algorithm 1 in the W-GAN paper. Note that compared to original GANs, we train the discriminator many more times than the generator. The reason behind that is the output of the discriminator serves as an estimation of the EM distance. We want to train the discriminator until it can closely estimate the EM distance. In order to make sure that the discriminator has a sufficient good estimation at the very beginning of the training, we even train it for 100 iterations before train the generator (this is disabled in fast mode because this will significantly take longer time)."
    ]
   },
   "source": [
    "### モデルを訓練する\n",
    "\n",
    "GAN を訓練するためのコードは、W-GAN 論文のアルゴリズム1に従順に従います。元々のGANと比較して、生成器よりも多くの回数、弁別器を訓練することに注意してください。その背後にある理由は、弁別器の出力を、EM 距離の推定として機能させるためです。近似的に EM 距離を推定できるまで、弁別を訓練しようとします。弁別器が訓練のごく初期でも十分良い推定ができることを保証するために、生成器を訓練する前に分別器を100回の反復訓練します。（これは大幅に長い時間がかかるので、高速モードで無効になっています。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_WGAN(reader_train, generator, discriminator):\n",
    "    X_real, X_fake, D_real, clipped_D_params, Z, G_trainer, D_trainer = \\\n",
    "        build_WGAN_graph(g_input_dim, d_input_dim, generator, discriminator)\n",
    "    # print out loss for each model for upto 25 times\n",
    "    \n",
    "    print_frequency_mbsize = num_minibatches // 25\n",
    "    \n",
    "    print(\"First row is Generator loss, second row is Discriminator loss\")\n",
    "    pp_G = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    pp_D = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    \n",
    "    input_map = {X_real: reader_train.streams.features}\n",
    "\n",
    "    for training_step in range(num_minibatches):\n",
    "        # train the discriminator model for diter steps\n",
    "        if not isFast and (training_step < 25 or training_step % 500 == 0):\n",
    "            diter = 100\n",
    "        else:\n",
    "            diter = 5\n",
    "        for d_train_step in range(diter):\n",
    "            for parameter, clipped in zip(D_real.parameters, clipped_D_params):\n",
    "                C.assign(parameter, clipped).eval()\n",
    "            Z_data = noise_sample(minibatch_size)\n",
    "            X_data = reader_train.next_minibatch(minibatch_size, input_map)\n",
    "            batch_inputs = {X_real: X_data[X_real].data, Z: Z_data}\n",
    "            D_trainer.train_minibatch(batch_inputs)\n",
    "        \n",
    "        Z_data = noise_sample(minibatch_size)\n",
    "        batch_inputs = {Z: Z_data}\n",
    "        G_trainer.train_minibatch(batch_inputs)\n",
    "       \n",
    "        pp_G.update_with_trainer(G_trainer)\n",
    "        pp_D.update_with_trainer(D_trainer)\n",
    "\n",
    "    G_trainer_loss = G_trainer.previous_minibatch_loss_average\n",
    "    return Z, X_fake, G_trainer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading map file: data/CIFAR-10/train_map.txt\n",
      "Generator input shape:  (100,)\n",
      "h0 shape (256, 4, 4)\n",
      "h1 shape (128, 8, 8)\n",
      "h2 shape : (64, 16, 16)\n",
      "h3 shape : (3, 32, 32)\n",
      "Discriminator convolution input shape (3, 32, 32)\n",
      "h0 shape : (64, 16, 16)\n",
      "h1 shape : (128, 8, 8)\n",
      "h2 shape : (256, 4, 4)\n",
      "h3 shape : (1,)\n",
      "First row is Generator loss, second row is Discriminator loss\n"
     ]
    }
   ],
   "source": [
    "reader_train = create_reader(train_file, True)\n",
    "G_input, G_output, G_trainer_loss = train_WGAN(reader_train,\n",
    "                                          convolutional_generator,\n",
    "                                          convolutional_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the generator loss \n",
    "print(\"Training loss of the generator is: {0:.2f}\".format(G_trainer_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "### Generating Fake (Synthetic) Images (W-GAN)\n",
     "Now that we have trained the model, we can create fake images simply by feeding random noise into the generator and displaying the outputs. Below are a few images generated from random samples. To get a new set of samples, you can re-run the last cell."
    ]
   },
   "source": [
    "### 偽 (合成) 画像の生成 (W-GAN)\n",
    "\n",
    "モデルを訓練したので、単に生成器にランダムノイズを渡し、出力を表示することによって偽の画像を作成することができます。以下は、ランダムなサンプルから生成されたいくつかの画像です。新しいサンプルのいくつかを取得するには、最後のセルを再実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_images(images, subplot_shape):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(*subplot_shape)\n",
    "    for image, ax in zip(images, axes.flatten()):\n",
    "        image = image[np.array([2,1,0]),:,:]\n",
    "        image = np.rollaxis(image / 2 + 0.5, 0, 3)\n",
    "        ax.imshow(image, vmin=-1.0, vmax=1.0)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "noise = noise_sample(36)\n",
    "images = G_output.eval({G_input: noise})\n",
    "plot_images(images, subplot_shape=[6, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "Larger number of iterations should generate more realistic looking images."
    ]
   },
   "source": [
    "反復回数の数が多いほど、より現実的な画像を生成できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "## LS-GAN Implementation\n",
     "\n",
     "Since the generator and discriminator architectures of LS-GAN is the same as W-GAN, we will reuse the generator and the discriminator we defined for W-GAN. The main difference between W-GAN and LS-GAN is their loss function and optimizer they use. We redefine the training parameters for LS-GAN."
    ]
   },
   "source": [
    "## LS-GAN 実装\n",
    "\n",
    "LS-GAN の生成器と弁別器のアーキテクチャは W-GAN と同じであるため、W-GAN のために定義された生成器と弁別器を再利用します。W-GAN と LS-GAN の主な違いは、使用する損失関数と最適化の方法です。LS-GAN のトレーニングパラメータを再定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training config\n",
    "minibatch_size = 64\n",
    "num_minibatches = 1000 if isFast else 20000\n",
    "lr = 0.0001\n",
    "momentum = 0.5\n",
    "lambda_ = 0.0002 # lambda in LS-GAN loss function, controls the size of margin\n",
    "weight_decay = 0.00005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "### Build the graph\n",
     "As we mentioned above, one of the differences between LS-GAN and W-GAN is there loss function. In `build_LSGAN_graph`, we should define the loss function for the generator and the discriminator. Another difference is that we do not do weight clipping in LS-GAN, so `clipped_D_parames` is no longer needed. Instead, we use weight decay which is mathematically equivalent to adding an l2 regularization in the optimizer."
    ]
   },
   "source": [
    "### グラフを作成する\n",
    "\n",
    "前述したように、LS-GAN と W-GAN の違いの一つは、損失関数です。`build_LSGAN_graph`では、生成器と弁別器の損失関数を定義する必要があります。もう一つの違いは、LS-GAN の重みのクリッピングをしないので、 `clipped_D_parames` はもはや必要ありません。代わりに、最適化で l2 正則化と数学的に等価な重み減衰を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_LSGAN_graph(noise_shape, image_shape, generator, discriminator):\n",
    "    \n",
    "    input_dynamic_axes = [C.Axis.default_batch_axis()]\n",
    "    Z = C.input_variable(noise_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real = C.input_variable(image_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real_scaled = (X_real - 127.5) / 127.5\n",
    "\n",
    "    # Create the model function for the generator and discriminator models\n",
    "    X_fake = generator(Z)\n",
    "    D_real = discriminator(X_real_scaled)\n",
    "    D_fake = D_real.clone(\n",
    "        method = 'share',\n",
    "        substitutions = {X_real_scaled.output: X_fake.output}\n",
    "    )\n",
    "    \n",
    "    G_loss = D_fake\n",
    "    D_loss = C.element_max(D_real - D_fake + lambda_ * C.reduce_sum(C.abs(X_fake - X_real_scaled)), [0.])\n",
    "    \n",
    "    G_learner = C.adam(\n",
    "            parameters = X_fake.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999),\n",
    "            l2_regularization_weight=weight_decay,\n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "    )\n",
    "            \n",
    "    D_learner = C.adam(\n",
    "            parameters = D_real.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999),\n",
    "            l2_regularization_weight=0.00005,\n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "        )\n",
    "    \n",
    "    # Instantiate the trainers\n",
    "    G_trainer = C.Trainer(X_fake,\n",
    "                        (G_loss, None),\n",
    "                        G_learner)\n",
    "    D_trainer = C.Trainer(D_real,\n",
    "                        (D_loss, None),\n",
    "                        D_learner)\n",
    "\n",
    "    return X_real, X_fake, Z, G_trainer, D_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "### Train the model\n",
     "To train the LS-GAN model, we can just simply update the discriminator and the generator alternatively at each round."
    ]
   },
   "source": [
    "### モデルを訓練する\n",
    "\n",
    "LS-GAN モデルを訓練するには、単に各ラウンドで交互に弁別器と生成器を更新するだけでできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LSGAN(reader_train, generator, discriminator):\n",
    "    X_real, X_fake, Z, G_trainer, D_trainer = \\\n",
    "        build_LSGAN_graph(g_input_dim, d_input_dim, generator, discriminator)\n",
    "        \n",
    "    # print out loss for each model for upto 25 times\n",
    "    print_frequency_mbsize = num_minibatches // 25\n",
    "    \n",
    "    print(\"First row is Generator loss, second row is Discriminator loss\")\n",
    "    pp_G = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    pp_D = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    \n",
    "    \n",
    "    input_map = {X_real: reader_train.streams.features}\n",
    "\n",
    "    for training_step in range(num_minibatches):\n",
    "        # Train the discriminator and the generator alternatively\n",
    "        Z_data = noise_sample(minibatch_size)\n",
    "        X_data = reader_train.next_minibatch(minibatch_size, input_map)\n",
    "        batch_inputs = {X_real: X_data[X_real].data, Z: Z_data}\n",
    "        D_trainer.train_minibatch(batch_inputs)\n",
    "        \n",
    "        Z_data = noise_sample(minibatch_size)\n",
    "        batch_inputs = {Z: Z_data}\n",
    "        G_trainer.train_minibatch(batch_inputs)\n",
    "        \n",
    "        pp_G.update_with_trainer(G_trainer)\n",
    "        pp_D.update_with_trainer(D_trainer)\n",
    "\n",
    "    G_trainer_loss = G_trainer.previous_minibatch_loss_average\n",
    "    return Z, X_fake, G_trainer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader_train = create_reader(train_file, True)\n",
    "G_input, G_output, G_trainer_loss = train_LSGAN(reader_train,\n",
    "                                          convolutional_generator,\n",
    "                                          convolutional_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the generator loss \n",
    "print(\"Training loss of the generator is: {0:.2f}\".format(G_trainer_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "### Generating Fake (Synthetic) Images (LS-GAN)\n",
     "Now that we have trained the LS-GAN model, we can create fake images simply by feeding random noise into the generator and displaying the outputs. Below are a few images generated from random samples. To get a new set of samples, you can re-run the last cell."
    ]
   },
   "source": [
    "### 偽 (合成) 画像の生成 (LS-GAN)\n",
    "\n",
    "LS-GAN モデルを訓練したので、単に生成器にランダムノイズを渡し、出力を表示することによって偽の画像を作成することができます。以下は、ランダムなサンプルから生成されたいくつかの画像です。新しいサンプルのいくつかを取得するには、最後のセルを再実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_images(images, subplot_shape):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(*subplot_shape)\n",
    "    for image, ax in zip(images, axes.flatten()):\n",
    "        image = image[np.array([2,1,0]),:,:]\n",
    "        image = np.rollaxis(image / 2 + 0.5, 0, 3)\n",
    "        ax.imshow(image, vmin=-1.0, vmax=1.0)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "noise = noise_sample(36)\n",
    "images = G_output.eval({G_input: noise})\n",
    "plot_images(images, subplot_shape=[6, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "Larger number of iterations should generate more realistic looking images. "
    ]
   },
   "source": [
    "反復回数の数が多いほど、より現実的な画像を生成できます。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [test-py35]",
   "language": "python",
   "name": "Python [test-py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
